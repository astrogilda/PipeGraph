{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nFinding number of clusters\n-----------------------------------------\n\n A two step PipeGraph is built by embedding a clustering algorithm such as Kmeans followed by\n a supervised classification algorithm such as Linear Discriminant Analysis. This kind of structure\n is useful in those scenarios in which the dataset comprises different subpopulations\n but the class labels are unknown. A common question that arises in such scenario is related to\n the best number of clusters. For answering that question, we might want to measure the quality\n of the clustering in accordance to the later capability of reproducing the results obtained by\n the clustering algorithm on a model based classification technique such as linear discriminant analysis.\n\n Such PipeGraph is fit using a GridSearchCV strategy and an artificial dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#%%\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pipegraph import PipeGraph\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns; sns.set()\n\nX, y = datasets.make_blobs(n_samples=10000, n_features=5, centers=10)\n\nclustering = KMeans(n_clusters=10)\n# The values of the parameters in the initialization do not affect the grid search.\n\nclassification = LinearDiscriminantAnalysis()\n\nsteps = [('clustering', clustering),\n         ('classification', classification)\n         ]\n\nmodel = PipeGraph(steps=steps)\nmodel.inject(sink='clustering',     sink_var='X', source='_External',  source_var='X')\nmodel.inject(sink='classification', sink_var='X', source='_External',  source_var='X')\nmodel.inject(sink='classification', sink_var='y', source='clustering', source_var='predict')\n\nn_folds = 5\nnumber_of_clusters_to_explore = np.arange(2, 20)\nsearch = GridSearchCV(model, param_grid=dict(clustering__n_clusters=number_of_clusters_to_explore), cv=n_folds)\nsearch.fit(X)\n#%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results show the CV scores along with a fill area pointing out the standard deviation bands.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Code inspired by http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-download-auto-examples-exercises-plot-cv-diabetes-py\n\nnumber_of_clusters = search.cv_results_['param_clustering__n_clusters'].data.astype(int)\nerrors = search.cv_results_['mean_test_score']\ndeviations = search.cv_results_['std_test_score']/np.sqrt(n_folds)\n\nplt.figure().set_size_inches(8, 6)\nplt.plot(number_of_clusters, errors)\nplt.plot(number_of_clusters, errors + deviations, 'b--')\nplt.plot(number_of_clusters, errors - deviations, 'b--')\nplt.fill_between(number_of_clusters, errors + deviations, errors - deviations, alpha=0.2)\nplt.ylabel('CV score +/- std error')\nplt.xlabel('Number of clusters')\nplt.axhline(np.max(errors), linestyle='--', color='.5')\nplt.xlim([number_of_clusters_to_explore[0], number_of_clusters_to_explore[-1]])\nplt.show()\n#%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results show an abrupt slope right after 10 clusters, which is commonly interpreted as\na hint for maximum number of clusters.\n\nThe accuracy and confusion matrix for the 10 clusters configuration are shown below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Code for plotting the confusion matrix taken from 'Python Data Science Handbook' by Jake VanderPlas\n\nmodel.fit(X)\nscore = model.score(X)\nprint('Accuracy:',score)\nkmeans_labels = model._predict_data[('clustering', 'predict')]\nprediction = model.predict(X)\n\nmat = confusion_matrix(y_true=kmeans_labels, y_pred=prediction)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('KMeans label')\nplt.ylabel('LDA predicted label');\nplt.show()\n#%%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}